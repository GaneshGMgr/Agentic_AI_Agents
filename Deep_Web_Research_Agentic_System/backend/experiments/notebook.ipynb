{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d98fb4e",
   "metadata": {},
   "source": [
    "## ❓ Testing Questions\n",
    "\n",
    "### 🧠 `research_paper` — Mentions academic/research sources  \n",
    "These should all be classified as **`research_paper`**:\n",
    "\n",
    "- \"Generate a report using academic studies on climate change.\"\n",
    "- \"Summarize the latest findings from research papers on AI ethics.\"\n",
    "- \"Prove your answer with scholarly articles.\"\n",
    "- \"I want evidence from research papers on the effectiveness of remote learning.\"\n",
    "- \"Provide insights from peer-reviewed studies about mental health.\"\n",
    "- \"Explain using research papers how sleep affects productivity.\"\n",
    "\n",
    "---\n",
    "\n",
    "### 📄 `report_generation` — General requests without needing academic sources  \n",
    "These should be classified as **`report_generation`**:\n",
    "\n",
    "- \"Write a report on the history of the internet.\"\n",
    "- \"Summarize the impact of social media on teenagers.\"\n",
    "- \"Generate a report about the benefits of exercise.\"\n",
    "- \"Create a short report on global warming for school.\"\n",
    "- \"Write a brief summary of renewable energy technologies.\"\n",
    "\n",
    "> Even though these sound formal, none ask for academic or research-backed material — so they should fall under `report_generation`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌐 `google_search` — Basic, factual, or current-event questions  \n",
    "These should be classified as **`google_search`**:\n",
    "\n",
    "- \"Who is the current Prime Minister of Canada?\"\n",
    "- \"What is the population of Tokyo?\"\n",
    "- \"Best smartphones under $500 in 2025.\"\n",
    "- \"When was the iPhone 16 released?\"\n",
    "- \"Where is the Eiffel Tower located?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce828327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ganes_3ck5\\DataScience\\Gen_AI\\Course_GenAI\\Gen_AI_In-Depth\\Agentic_AI_Agents\\Deep_Web_Research_Agentic_System\\venv\\Lib\\site-packages\\scholarly\\_scholarly.py:312: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  m = re.search(\"cites=[\\d+,]*\", object[\"citedby_url\"])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from scholarly import scholarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from autogen import AssistantAgent\n",
    "from utils.config_loader import load_config # yaml loader\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ed46f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaacf7f",
   "metadata": {},
   "source": [
    "### Models Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a4ffca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigLoader:\n",
    "    def __init__(self):\n",
    "        print(\"Loading config...\")\n",
    "        self.config = load_config()\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.config[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60896631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader(BaseModel): # BaseModel helps with data validation, settings management, and more \n",
    "    # it can only be one of these exact strings: \"groq\", \"openai\", \"ollama-deepseek\", \"ollama-llama3\", or \"ollama-mistral\"\n",
    "    # It acts like a validation check: if you try to create a ModelLoader instance with some other string as model_key, Pydantic will raise an error.\n",
    "    model_key: Literal[\n",
    "        \"groq\", \n",
    "        \"ollama-deepseek\", \n",
    "        \"ollama-llama3\", \n",
    "        \"ollama-mistral\"\n",
    "    ] = \"ollama-llama3\" # default is ollama-llama3\n",
    "\n",
    "    config: ConfigLoader = Field(default_factory=ConfigLoader, exclude=True)\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def load_llm(self):\n",
    "        print(\"LLM loading...\")\n",
    "        print(f\"Loading model with config key: {self.model_key}\")\n",
    "\n",
    "        # Read provider and model_name dynamically from config\n",
    "        provider = self.config[\"llm\"][self.model_key][\"provider\"]\n",
    "        model_name = self.config[\"llm\"][self.model_key][\"model_name\"]\n",
    "\n",
    "        if provider == \"groq\":\n",
    "            groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "            print(f\"Using Groq model: {model_name}\")\n",
    "            return ChatGroq(model=model_name, api_key=groq_api_key)\n",
    "\n",
    "\n",
    "        elif provider == \"ollama\":\n",
    "            print(f\"Using Ollama model: {model_name}\")\n",
    "            return OllamaLLM(model=model_name)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d0b01",
   "metadata": {},
   "source": [
    "### tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0e0cb",
   "metadata": {},
   "source": [
    "### 1.QueryClassifierAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40f39c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from prompt_library.prompt import query_classifier_prompt\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "class QueryClassifierAgent:\n",
    "    def __init__(self, model_name: str = \"llama3.2:latest\"):\n",
    "        self.llm = OllamaLLM(model=model_name)\n",
    "        self.chain = query_classifier_prompt | self.llm\n",
    "\n",
    "    def classify(self, query: str) -> str:\n",
    "        response = self.chain.invoke({\"query\": query})\n",
    "        return response.strip()\n",
    "    \n",
    "\n",
    "def classify_query_tool(query: str) -> str:\n",
    "    agent = QueryClassifierAgent()\n",
    "    return agent.classify(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5452e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_classifier_tool = Tool(\n",
    "    name=\"query_classifier\",\n",
    "    description=\"Classifies user queries into categories like google_search, research_paper, report_generation, or unknown.\",\n",
    "    func=classify_query_tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce08ff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What's your favorite programming language?\n",
      "Predicted Category: google_search\n"
     ]
    }
   ],
   "source": [
    "agent = QueryClassifierAgent()\n",
    "\n",
    "# query_google_search_1 = \"What is the capital of Argentina?\"\n",
    "# category_1 = agent.classify(query_google_search_1)\n",
    "# print(\"Query:\", query_google_search_1)\n",
    "# print(\"Predicted Category:\", category_1)\n",
    "\n",
    "# query_google_search_2 = \"How do I reset my iPhone?\"\n",
    "# category_2 = agent.classify(query_google_search_2)\n",
    "# print(\"Query:\", query_google_search_2)\n",
    "# print(\"Predicted Category:\", category_2)\n",
    "\n",
    "# query_research_paper_1 = \"Can you list recent research papers on reinforcement learning?\"\n",
    "# category_3 = agent.classify(query_research_paper_1)\n",
    "# print(\"Query:\", query_research_paper_1)\n",
    "# print(\"Predicted Category:\", category_3)\n",
    "\n",
    "# query_research_paper_2 = \"What are the latest studies on climate change and food security?\"\n",
    "# category_4 = agent.classify(query_research_paper_2)\n",
    "# print(\"Query:\", query_research_paper_2)\n",
    "# print(\"Predicted Category:\", category_4)\n",
    "\n",
    "# query_report_1 = \"Generate a report on the growth of the electric vehicle market in India.\"\n",
    "# category_5 = agent.classify(query_report_1)\n",
    "# print(\"Query:\", query_report_1)\n",
    "# print(\"Predicted Category:\", category_5)\n",
    "\n",
    "# query_report_2 = \"Summarize a report about cyber threats in healthcare.\"\n",
    "# category_6 = agent.classify(query_report_2)\n",
    "# print(\"Query:\", query_report_2)\n",
    "# print(\"Predicted Category:\", category_6)\n",
    "\n",
    "# query_unknown_1 = \"Tell me a joke about quantum mechanics.\"\n",
    "# category_7 = agent.classify(query_unknown_1)\n",
    "# print(\"Query:\", query_unknown_1)\n",
    "# print(\"Predicted Category:\", category_7)\n",
    "\n",
    "query_unknown_2 = \"What's your favorite programming language?\"\n",
    "category_8 = agent.classify(query_unknown_2)\n",
    "print(\"Query:\", query_unknown_2)\n",
    "print(\"Predicted Category:\", category_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417afd5",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c62e091",
   "metadata": {},
   "source": [
    "USER\n",
    " │\n",
    " ▼\n",
    "┌────────────────────┐\n",
    "│ QueryClassifierAgent│\n",
    "└────────────────────┘\n",
    "        │\n",
    "        ├───────────────┬────────────────────┬────────────────────────────┐\n",
    "        │               │                    │                            │\n",
    "        ▼               ▼                    ▼                            ▼\n",
    "\"google_search\"   \"report_generation\"   \"research_paper\"             \"unknown\"\n",
    "        │               │                    │                            │\n",
    "        │               ▼                    ▼                            ▼\n",
    "        │    ┌────────────────────┐   ┌────────────────────┐     ┌────────────────────┐\n",
    "        │    │QuestionDecomposerAgent│   │ResearchClassifierAgent│     │ UnknownHandlerAgent │\n",
    "        │    └────────────────────┘   └────────────────────┘     └────────────────────┘\n",
    "        │               │                    │\n",
    "        │               ▼                    ├────────────┬───────────────┬─────────────┐\n",
    "        │    ┌────────────────────┐   ▼            ▼               ▼             ▼\n",
    "        │    │ GoogleSearchAgent  │ \"preprint\"  \"biomedical\"  \"multidisciplinary\"\n",
    "        │    └────────────────────┘    │            │               │\n",
    "        │               │              ▼            ▼               ▼\n",
    "        │               │    ┌────────────────┐ ┌────────────────┐ ┌────────────────────────┐\n",
    "        │               │    │ PreprintAgent  │ │ BiomedicalAgent│ │ MultidisciplinaryAgent │\n",
    "        │               │    └────────────────┘ └────────────────┘ └────────────────────────┘\n",
    "        │               │         │              │                    │\n",
    "        │               │         └──────┬───────┴────────┬───────────┘\n",
    "        │               │                ▼                ▼\n",
    "        │               │      ┌────────────────────────────┐\n",
    "        │               │      │   ReportGeneratorAgent     │\n",
    "        │               │      └────────────────────────────┘\n",
    "        │               │                │\n",
    "        └───────────────┴────────────────┘\n",
    "                         ▼\n",
    "                     Final Response\n",
    "                         │\n",
    "                         ▼\n",
    "                       USER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc1c98",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Detailed Flow\n",
    "\n",
    "1. **User Query** arrives and is passed to `QueryClassifierAgent`.\n",
    "\n",
    "2. **QueryClassifierAgent** classifies intent into:\n",
    "   - `\"google_search\"`: Routes directly to `GoogleSearchAgent`.\n",
    "   - `\"report_generation\"`: Routes to `QuestionDecomposerAgent`.\n",
    "   - `\"research_paper\"`: Routes to `ResearchClassifierAgent`.\n",
    "   - `\"unknown\"`: Routes to `UnknownHandlerAgent`.\n",
    "\n",
    "3. If **google_search**:\n",
    "   - `GoogleSearchAgent` executes search and returns results to the user.\n",
    "\n",
    "4. If **report_generation**:\n",
    "   - `QuestionDecomposerAgent` breaks down query into sub-questions.\n",
    "   - Sub-questions passed to `GoogleSearchAgent` for info retrieval.\n",
    "   - Results forwarded to `ReportGeneratorAgent`.\n",
    "   - Report synthesized and returned.\n",
    "\n",
    "5. If **research_paper**:\n",
    "   - `ResearchClassifierAgent` classifies domain into:\n",
    "     - `preprint` → `PreprintAgent`\n",
    "     - `biomedical` → `BiomedicalAgent`\n",
    "     - `multidisciplinary` → `MultidisciplinaryAgent`\n",
    "   - Selected agent fetches papers.\n",
    "   - Papers passed to `ReportGeneratorAgent`.\n",
    "   - Final report returned.\n",
    "\n",
    "6. If **unknown**:\n",
    "   - `UnknownHandlerAgent` sends default message to user.\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Agents use LLMs with specialized prompts.\n",
    "- The graph is orchestrated with LangGraph `StateGraph`.\n",
    "- Modular design allows adding/removing agents easily.\n",
    "- Supports multi-source paper fetching, topic expansion, and report generation.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Architecture Documentation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be6320",
   "metadata": {},
   "source": [
    "USER\n",
    " │\n",
    " ▼\n",
    "┌────────────────────┐\n",
    "│ QueryClassifierAgent│\n",
    "└────────────────────┘\n",
    "        │\n",
    "        ├───────────────┬────────────────────┬────────────────────────────┐\n",
    "        │               │                    │                            │\n",
    "        ▼               ▼                    ▼                            ▼\n",
    "\"google_search\"   \"report_generation\"   \"research_paper\"             \"unknown\"\n",
    "        │               │                    │                            │\n",
    "        │               ▼                    │                            ▼\n",
    "        │    ┌────────────────────┐         │                    ┌────────────────────┐\n",
    "        │    │QuestionDecomposerAgent│       │                    │ UnknownHandlerAgent │\n",
    "        │    └────────────────────┘         │                    └────────────────────┘\n",
    "        │               │                    │\n",
    "        │               ▼                    ▼\n",
    "        │    ┌────────────────────┐   ┌────────────────────┐\n",
    "        │    │ GoogleSearchAgent  │   │ResearchClassifierAgent│\n",
    "        │    └────────────────────┘   └────────────────────┘\n",
    "        │               │                    │\n",
    "        │               │        ┌────────────┬───────────────┬─────────────┐\n",
    "        │               │        │            │               │             │\n",
    "        │               │        ▼            ▼               ▼             ▼\n",
    "        │               │   \"preprint\"  \"biomedical\"  \"multidisciplinary\"\n",
    "        │               │        │            │               │\n",
    "        │               │        ▼            ▼               ▼\n",
    "        │               │  ┌─────────────┐ ┌───────────────┐ ┌────────────────────────┐\n",
    "        │               │  │ PreprintAgent│ │ BiomedicalAgent│ │ MultidisciplinaryAgent │\n",
    "        │               │  └─────────────┘ └───────────────┘ └────────────────────────┘\n",
    "        │               │        │            │               │\n",
    "        │               │        └──────┬─────┴────────┬──────┘\n",
    "        │               │               ▼             ▼\n",
    "        │               │      ┌────────────────────────────┐\n",
    "        │               │      │   ReportGeneratorAgent     │\n",
    "        │               │      └────────────────────────────┘\n",
    "        │               │               │\n",
    "        └───────────────┴───────────────┘\n",
    "                        ▼\n",
    "                 Final Response\n",
    "                        │\n",
    "                        ▼\n",
    "                      USER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb8f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchAgents:\n",
    "    def __init__(self, model_loader: ModelLoader):\n",
    "        self.llm = model_loader.load_llm()  # Get the actual LLM instance (ChatGroq or OllamaLLM)\n",
    "\n",
    "        # Summarizer Agent\n",
    "        self.summarizer_agent = AssistantAgent(\n",
    "            name=\"summarizer_agent\",\n",
    "            system_message=(\n",
    "                \"Summarize the retrieved research papers and present concise summaries to the user. \"\n",
    "                \"JUST GIVE THE RELEVANT SUMMARIES OF THE RESEARCH PAPER AND NOT YOUR THOUGHT PROCESS.\"\n",
    "            ),\n",
    "            llm=self.llm,  # Use the loaded model instance here\n",
    "            human_input_mode=\"NEVER\",\n",
    "            code_execution_config=False\n",
    "        )\n",
    "\n",
    "        # Advantages and Disadvantages Agent\n",
    "        self.advantages_disadvantages_agent = AssistantAgent(\n",
    "            name=\"advantages_disadvantages_agent\",\n",
    "            system_message=(\n",
    "                \"Analyze the summaries of the research papers and provide a list of advantages and disadvantages \"\n",
    "                \"for each paper in a pointwise format. JUST GIVE THE ADVANTAGES AND DISADVANTAGES, NOT YOUR THOUGHT PROCESS.\"\n",
    "            ),\n",
    "            llm=self.llm,\n",
    "            human_input_mode=\"NEVER\",\n",
    "            code_execution_config=False\n",
    "        )\n",
    "\n",
    "        # Search Agent (optional)\n",
    "        self.search_agent = AssistantAgent(\n",
    "            name=\"search_agent\",\n",
    "            system_message=\"Suggest 3 related research topics for a given query.\",\n",
    "            llm=self.llm,\n",
    "            human_input_mode=\"NEVER\",\n",
    "            code_execution_config=False\n",
    "        )\n",
    "\n",
    "    def summarize_paper(self, paper_summary):\n",
    "        \"\"\"Generates a summary of the research paper.\"\"\"\n",
    "        summary_response = self.summarizer_agent.generate_reply(\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Summarize this paper: {paper_summary}\"}]\n",
    "        )\n",
    "        return summary_response.get(\"content\", \"Summarization failed!\") if isinstance(summary_response, dict) else str(summary_response)\n",
    "\n",
    "    def analyze_advantages_disadvantages(self, paper_summary):\n",
    "        \"\"\"Analyzes advantages and disadvantages of the research paper.\"\"\"\n",
    "        adv_dis_response = self.advantages_disadvantages_agent.generate_reply(\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Analyze advantages and disadvantages of this paper: {paper_summary}\"}]\n",
    "        )\n",
    "        return adv_dis_response.get(\"content\", \"Analysis failed!\") if isinstance(adv_dis_response, dict) else str(adv_dis_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2931b4",
   "metadata": {},
   "source": [
    "### Multidisciplinary Research Platforms\n",
    "These cover a broad range of subjects across many academic fields.\n",
    "\n",
    "Scholarly (Google Scholar scraping)\n",
    "\n",
    "ScienceDirect (Elsevier)\n",
    "\n",
    "Semantic Scholar API\n",
    "\n",
    "Why? They span a wide spectrum of disciplines — from engineering and social sciences to biomedical and computer science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d09559",
   "metadata": {},
   "source": [
    "[User Query]\n",
    "     ↓\n",
    "[QueryClassifierAgent]  ←– Classifies intent (search, research, report, etc.)\n",
    "     ↓\n",
    " ┌──────────────┬────────────────────┬─────────────┐\n",
    " ↓              ↓                    ↓\n",
    "[GoogleSearch] [ResearchAgent]    [ReportGeneratorAgent]\n",
    "                  ↓                        ↓\n",
    "       ┌──────────┬────────────────────┬─────────────────────┐\n",
    "       ↓          ↓                    ↓\n",
    "  [Preprint]  [Multidisciplinary]   [Biomedical & Life Sciences]\n",
    "   (arXiv)     (Scholar, SD, SS)     (PubMed, Springer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9208f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from scholarly import scholarly  # Make sure you have scholarly installed\n",
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "load_dotenv()\n",
    "ScienceDirect_Elsevier_API = os.getenv(\"ScienceDirect_Elsevier_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "class MultidisciplinaryResearchPlatforms:\n",
    "    def __init__(self, search_agent=None, sciencedirect_api_key=None):\n",
    "        print(\"MultidisciplinaryResearchPlatforms Init\")\n",
    "        self.search_agent = search_agent  # Optional LLM-based related topics generator\n",
    "        self.sciencedirect_api_key = sciencedirect_api_key  # For Elsevier API\n",
    "\n",
    "    def fetch_google_scholar_papers(self, query: str) -> List[Dict]: # Scholarly (Google Scholar scraping)\n",
    "        \"\"\"\n",
    "        Fetches top 5 research papers from Google Scholar.\n",
    "        If fewer than 5 papers are found, expands the search using related topics.\n",
    "        Returns:\n",
    "            list: A list of dictionaries containing paper details (title, summary, link).\n",
    "        \"\"\"\n",
    "        papers = []\n",
    "        search_results = scholarly.search_pubs(query)\n",
    "\n",
    "        for i, paper in enumerate(search_results):\n",
    "            if i >= 5:\n",
    "                break\n",
    "            papers.append({\n",
    "                \"title\": paper[\"bib\"][\"title\"],\n",
    "                \"summary\": paper[\"bib\"].get(\"abstract\", \"No summary available\"),\n",
    "                \"link\": paper.get(\"pub_url\", \"No link available\")\n",
    "            })\n",
    "\n",
    "        # Expand search if fewer than 5 papers\n",
    "        if len(papers) < 5 and self.search_agent:\n",
    "            related_topics_response = self.search_agent.generate_reply(\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Suggest 3 related research topics for '{query}'\"}]\n",
    "            )\n",
    "            related_topics = related_topics_response.get(\"content\", \"\").split(\"\\n\")\n",
    "            for topic in related_topics:\n",
    "                topic = topic.strip()\n",
    "                if topic and len(papers) < 5:\n",
    "                    new_papers = scholarly.search_pubs(topic)\n",
    "                    for i, paper in enumerate(new_papers):\n",
    "                        if len(papers) >= 5:\n",
    "                            break\n",
    "                        papers.append({\n",
    "                            \"title\": paper[\"bib\"][\"title\"],\n",
    "                            \"summary\": paper[\"bib\"].get(\"abstract\", \"No summary available\"),\n",
    "                            \"link\": paper.get(\"pub_url\", \"No link available\")\n",
    "                        })\n",
    "        return papers\n",
    "\n",
    "    def fetch_sciencedirect_papers(self, query: str) -> List[Dict]: # ScienceDirect (Elsevier)\n",
    "        \"\"\"\n",
    "        Fetches top 5 papers from ScienceDirect via Elsevier API.\n",
    "        Requires an API key.\n",
    "        \"\"\"\n",
    "        if not self.ScienceDirect_Elsevier_API:\n",
    "            raise ValueError(\"ScienceDirect API key not provided.\")\n",
    "\n",
    "        headers = {\n",
    "            \"X-ELS-APIKey\": self.ScienceDirect_Elsevier_API,\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"count\": 5,\n",
    "            \"sort\": \"relevance\"\n",
    "        }\n",
    "\n",
    "        url = \"https://api.elsevier.com/content/search/sciencedirect\"\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        papers = []\n",
    "        if response.status_code == 200:\n",
    "            results = response.json()\n",
    "            entries = results.get(\"search-results\", {}).get(\"entry\", [])\n",
    "            for entry in entries:\n",
    "                papers.append({\n",
    "                    \"title\": entry.get(\"dc:title\", \"No title\"),\n",
    "                    \"summary\": entry.get(\"dc:description\", \"No summary available\"),\n",
    "                    \"link\": entry.get(\"prism:url\", \"No link available\")\n",
    "                })\n",
    "\n",
    "        # Expand search if fewer than 5 papers\n",
    "        if len(papers) < 5 and self.search_agent:\n",
    "            related_topics_response = self.search_agent.generate_reply(\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Suggest 3 related research topics for '{query}'\"}]\n",
    "            )\n",
    "            related_topics = related_topics_response.get(\"content\", \"\").split(\"\\n\")\n",
    "            for topic in related_topics:\n",
    "                topic = topic.strip()\n",
    "                if topic and len(papers) < 5:\n",
    "                    params[\"query\"] = topic\n",
    "                    response = requests.get(url, headers=headers, params=params)\n",
    "                    if response.status_code == 200:\n",
    "                        results = response.json()\n",
    "                        entries = results.get(\"search-results\", {}).get(\"entry\", [])\n",
    "                        for entry in entries:\n",
    "                            if len(papers) >= 5:\n",
    "                                break\n",
    "                            papers.append({\n",
    "                                \"title\": entry.get(\"dc:title\", \"No title\"),\n",
    "                                \"summary\": entry.get(\"dc:description\", \"No summary available\"),\n",
    "                                \"link\": entry.get(\"prism:url\", \"No link available\")\n",
    "                            })\n",
    "        return papers\n",
    "\n",
    "    def fetch_semantic_scholar_papers(self, query: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetches top 5 papers from Semantic Scholar API.\n",
    "        \"\"\"\n",
    "        url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"limit\": 5,\n",
    "            \"fields\": \"title,abstract,url\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        papers = []\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            for paper in data.get(\"data\", []):\n",
    "                papers.append({\n",
    "                    \"title\": paper.get(\"title\", \"No title\"),\n",
    "                    \"summary\": paper.get(\"abstract\", \"No summary available\"),\n",
    "                    \"link\": paper.get(\"url\", \"No link available\")\n",
    "                })\n",
    "\n",
    "        # Expand search if fewer than 5 papers\n",
    "        if len(papers) < 5 and self.search_agent:\n",
    "            related_topics_response = self.search_agent.generate_reply(\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Suggest 3 related research topics for '{query}'\"}]\n",
    "            )\n",
    "            related_topics = related_topics_response.get(\"content\", \"\").split(\"\\n\")\n",
    "            for topic in related_topics:\n",
    "                topic = topic.strip()\n",
    "                if topic and len(papers) < 5:\n",
    "                    params[\"query\"] = topic\n",
    "                    params[\"limit\"] = 5 - len(papers)\n",
    "                    response = requests.get(url, params=params)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        for paper in data.get(\"data\", []):\n",
    "                            if len(papers) >= 5:\n",
    "                                break\n",
    "                            papers.append({\n",
    "                                \"title\": paper.get(\"title\", \"No title\"),\n",
    "                                \"summary\": paper.get(\"abstract\", \"No summary available\"),\n",
    "                                \"link\": paper.get(\"url\", \"No link available\")\n",
    "                            })\n",
    "        return papers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41dd20",
   "metadata": {},
   "source": [
    "### 2. Preprint & Early-stage Research Repositories\n",
    "These focus on sharing preliminary findings before formal peer review.\n",
    "\n",
    "arXiv API\n",
    "\n",
    "Why? arXiv hosts preprints primarily in physics, math, computer science, and related quantitative fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd97e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832bc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "class PreprintEarlyStageResearchRepositories:\n",
    "    def __init__(self, search_agent=None):\n",
    "        print(\"DataLoader Init\")\n",
    "        self.search_agent = search_agent  # Allow search agent to be passed, if needed\n",
    "\n",
    "    def fetch_arxiv_papers(self, query):\n",
    "        \"\"\"\n",
    "            Fetches top 5 research papers from ArXiv based on the user query.\n",
    "            If <5 papers are found, expands the search using related topics.\n",
    "            \n",
    "            Returns:\n",
    "                list: A list of dictionaries containing paper details (title, summary, link).\n",
    "        \"\"\"\n",
    "        \n",
    "        def search_arxiv(query):  # querying the arXiv API to fetch research papers based on a given query\n",
    "            \"\"\"Helper function to query ArXiv API.\"\"\"\n",
    "            url = f\"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results=5\"\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                root = ET.fromstring(response.text)  # Converts XML string into an ElementTree object\n",
    "                return [\n",
    "                    {  # {http://www.w3.org/2005/Atom} is a namespace required for parsing XML correctly\n",
    "                        \"title\": entry.find(\"{http://www.w3.org/2005/Atom}title\").text,\n",
    "                        \"summary\": entry.find(\"{http://www.w3.org/2005/Atom}summary\").text,\n",
    "                        \"link\": entry.find(\"{http://www.w3.org/2005/Atom}id\").text\n",
    "                    }\n",
    "                    for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\")\n",
    "                ]\n",
    "            return []\n",
    "\n",
    "        papers = search_arxiv(query) # called automatically during the execution of fetch_arxiv_papers(query)\n",
    "\n",
    "        if len(papers) < 5 and self.search_agent:  # If fewer than 5 papers and self.search_agent exists, expand search\n",
    "            # self.search_agent.generate_reply() is likely an LLM-based agent (e.g., GPT) that generates 3 related research topics for \n",
    "            # the given query.\n",
    "            related_topics_response = self.search_agent.generate_reply(  # Ask the search agent for related research topics\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Suggest 3 related research topics for '{query}'\"}]\n",
    "            )\n",
    "            related_topics = related_topics_response.get(\"content\", \"\").split(\"\\n\")  # Extracts response and splits by newline into list\n",
    "\n",
    "            for topic in related_topics:\n",
    "                topic = topic.strip()  # Remove extra spaces\n",
    "                if topic and len(papers) < 5:\n",
    "                    new_papers = search_arxiv(topic)  # Again search_arxiv() for this 3 related research topics generated by LLM-based.\n",
    "                    papers.extend(new_papers)  # Add new papers to the list\n",
    "                    papers = papers[:5]  # Ensure max 5 papers\n",
    "\n",
    "        return papers\n",
    "\n",
    "    def fetch_google_scholar_papers(self, query):\n",
    "        \"\"\"\n",
    "        Fetches top 5 research papers from Google Scholar.\n",
    "        If fewer than 5 papers are found, expands the search using related topics.\n",
    "        Returns:\n",
    "            list: A list of dictionaries containing paper details (title, summary, link).\n",
    "        \"\"\"\n",
    "        papers = []\n",
    "        search_results = scholarly.search_pubs(query) #  search for research papers on Google Schola\n",
    "\n",
    "        # Get the first 5 papers from Google Scholar\n",
    "        for i, paper in enumerate(search_results):\n",
    "            if i >= 5:\n",
    "                break\n",
    "            papers.append({\n",
    "                \"title\": paper[\"bib\"][\"title\"],\n",
    "                \"summary\": paper[\"bib\"].get(\"abstract\", \"No summary available\"),\n",
    "                \"link\": paper.get(\"pub_url\", \"No link available\")\n",
    "            })\n",
    "        \n",
    "        # If fewer than 5 papers, expand search using related topics\n",
    "        if len(papers) < 5 and self.search_agent:  # Assuming self.search_agent is defined\n",
    "            # Use the search agent to suggest related research topics\n",
    "            related_topics_response = self.search_agent.generate_reply(\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Suggest 3 related research topics for '{query}'\"}]\n",
    "            )\n",
    "            related_topics = related_topics_response.get(\"content\", \"\").split(\"\\n\")\n",
    "\n",
    "            for topic in related_topics:\n",
    "                topic = topic.strip()  # Clean up the topic text\n",
    "                if topic and len(papers) < 5:\n",
    "                    # Re-query Google Scholar using the new related topic\n",
    "                    new_papers = scholarly.search_pubs(topic)\n",
    "                    for i, paper in enumerate(new_papers):\n",
    "                        if len(papers) >= 5:\n",
    "                            break\n",
    "                        papers.append({\n",
    "                            \"title\": paper[\"bib\"][\"title\"],\n",
    "                            \"summary\": paper[\"bib\"].get(\"abstract\", \"No summary available\"),\n",
    "                            \"link\": paper.get(\"pub_url\", \"No link available\")\n",
    "                        })\n",
    "\n",
    "        return papers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d24bb1",
   "metadata": {},
   "source": [
    "### 3. Biomedical and Life Sciences Databases\n",
    "These specialize in health, medicine, and biological sciences.\n",
    "\n",
    "PubMed\n",
    "\n",
    "Springer Nature Open Access API (with a strong presence in medical and life sciences)\n",
    "\n",
    "Why? PubMed is a biomedical powerhouse, while Springer Nature provides many peer-reviewed medical and biological journals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e48b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List, Dict\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "@tool\n",
    "class BiomedicalResearchPlatforms:\n",
    "    def __init__(self, search_agent=None, springer_api_key=None):\n",
    "        print(\"BiomedicalResearchPlatforms Init\")\n",
    "        self.search_agent = search_agent\n",
    "        self.springer_api_key = springer_api_key\n",
    "\n",
    "    def fetch_pubmed_papers(self, query: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetches top 5 papers from PubMed using NCBI E-utilities.\n",
    "        \"\"\"\n",
    "        base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "        search_url = f\"{base_url}esearch.fcgi\"\n",
    "        fetch_url = f\"{base_url}efetch.fcgi\"\n",
    "\n",
    "        params = {\n",
    "            \"db\": \"pubmed\",\n",
    "            \"term\": query,\n",
    "            \"retmode\": \"xml\",\n",
    "            \"retmax\": 5\n",
    "        }\n",
    "\n",
    "        response = requests.get(search_url, params=params)\n",
    "        ids = []\n",
    "        if response.status_code == 200:\n",
    "            root = ET.fromstring(response.text)\n",
    "            ids = [id_elem.text for id_elem in root.findall(\".//Id\")]\n",
    "\n",
    "        papers = []\n",
    "\n",
    "        if ids:\n",
    "            fetch_params = {\n",
    "                \"db\": \"pubmed\",\n",
    "                \"id\": \",\".join(ids),\n",
    "                \"retmode\": \"xml\"\n",
    "            }\n",
    "            fetch_response = requests.get(fetch_url, params=fetch_params)\n",
    "            if fetch_response.status_code == 200:\n",
    "                fetch_root = ET.fromstring(fetch_response.text)\n",
    "                for article in fetch_root.findall(\".//PubmedArticle\"):\n",
    "                    title_elem = article.find(\".//ArticleTitle\")\n",
    "                    abstract_elem = article.find(\".//AbstractText\")\n",
    "                    link = f\"https://pubmed.ncbi.nlm.nih.gov/{article.find('.//PMID').text}/\"\n",
    "\n",
    "                    papers.append({\n",
    "                        \"title\": title_elem.text if title_elem is not None else \"No title\",\n",
    "                        \"summary\": abstract_elem.text if abstract_elem is not None else \"No abstract\",\n",
    "                        \"link\": link\n",
    "                    })\n",
    "\n",
    "        # Expand search if needed\n",
    "        if len(papers) < 5 and self.search_agent:\n",
    "            related_topics = self._get_related_topics(query)\n",
    "            for topic in related_topics:\n",
    "                topic = topic.strip()\n",
    "                if topic and len(papers) < 5:\n",
    "                    params[\"term\"] = topic\n",
    "                    response = requests.get(search_url, params=params)\n",
    "                    if response.status_code == 200:\n",
    "                        root = ET.fromstring(response.text)\n",
    "                        ids = [id_elem.text for id_elem in root.findall(\".//Id\")]\n",
    "                        if ids:\n",
    "                            fetch_params[\"id\"] = \",\".join(ids)\n",
    "                            fetch_response = requests.get(fetch_url, params=fetch_params)\n",
    "                            if fetch_response.status_code == 200:\n",
    "                                fetch_root = ET.fromstring(fetch_response.text)\n",
    "                                for article in fetch_root.findall(\".//PubmedArticle\"):\n",
    "                                    if len(papers) >= 5:\n",
    "                                        break\n",
    "                                    title_elem = article.find(\".//ArticleTitle\")\n",
    "                                    abstract_elem = article.find(\".//AbstractText\")\n",
    "                                    link = f\"https://pubmed.ncbi.nlm.nih.gov/{article.find('.//PMID').text}/\"\n",
    "\n",
    "                                    papers.append({\n",
    "                                        \"title\": title_elem.text if title_elem is not None else \"No title\",\n",
    "                                        \"summary\": abstract_elem.text if abstract_elem is not None else \"No abstract\",\n",
    "                                        \"link\": link\n",
    "                                    })\n",
    "\n",
    "        return papers\n",
    "\n",
    "    def fetch_springer_papers(self, query: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetches top 5 papers from Springer Nature Open Access API.\n",
    "        \"\"\"\n",
    "        if not self.springer_api_key:\n",
    "            raise ValueError(\"Springer API key is required.\")\n",
    "\n",
    "        base_url = \"https://api.springernature.com/openaccess/jats\"\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"api_key\": self.springer_api_key,\n",
    "            \"p\": 5\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, params=params)\n",
    "        papers = []\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            for record in data.get(\"records\", []):\n",
    "                papers.append({\n",
    "                    \"title\": record.get(\"title\", \"No title\"),\n",
    "                    \"summary\": record.get(\"abstract\", \"No abstract available\"),\n",
    "                    \"link\": record.get(\"url\", [{\"value\": \"No link\"}])[0][\"value\"]\n",
    "                })\n",
    "\n",
    "        # Expand search if needed\n",
    "        if len(papers) < 5 and self.search_agent:\n",
    "            related_topics = self._get_related_topics(query)\n",
    "            for topic in related_topics:\n",
    "                topic = topic.strip()\n",
    "                if topic and len(papers) < 5:\n",
    "                    params[\"q\"] = topic\n",
    "                    response = requests.get(base_url, params=params)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        for record in data.get(\"records\", []):\n",
    "                            if len(papers) >= 5:\n",
    "                                break\n",
    "                            papers.append({\n",
    "                                \"title\": record.get(\"title\", \"No title\"),\n",
    "                                \"summary\": record.get(\"abstract\", \"No abstract available\"),\n",
    "                                \"link\": record.get(\"url\", [{\"value\": \"No link\"}])[0][\"value\"]\n",
    "                            })\n",
    "\n",
    "        return papers\n",
    "\n",
    "    def _get_related_topics(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Uses the search_agent to suggest related research topics.\n",
    "        \"\"\"\n",
    "        if not self.search_agent:\n",
    "            return []\n",
    "\n",
    "        related_topics_response = self.search_agent.generate_reply(\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Suggest 3 related research topics for '{query}'\"}]\n",
    "        )\n",
    "        return related_topics_response.get(\"content\", \"\").split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f739679a",
   "metadata": {},
   "source": [
    "## Our Google: Search Agent\n",
    "\n",
    "Given a Search term, search for it on the internet and summarize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457ac67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "# See note above about cost of WebSearchTool\n",
    "\n",
    "HOW_MANY_SEARCHES = 5\n",
    "\n",
    "INSTRUCTIONS = f\"You are a helpful research assistant. Given a query, come up with a set of web searches \\\n",
    "to perform to best answer the query. Output {HOW_MANY_SEARCHES} terms to query for.\"\n",
    "\n",
    "# We use Pydantic objects to describe the Schema of the output\n",
    "\n",
    "class WebSearchItem(BaseModel):\n",
    "    reason: str\n",
    "    \"Your reasoning for why this search is important to the query.\"\n",
    "\n",
    "    query: str\n",
    "    \"The search term to use for the web search.\"\n",
    "\n",
    "\n",
    "class WebSearchPlan(BaseModel):\n",
    "    searches: list[WebSearchItem]\n",
    "    \"\"\"A list of web searches to perform to best answer the query.\"\"\"\n",
    "\n",
    "# We pass in the Pydantic object to ensure the output follows the schema\n",
    "\n",
    "planner_agent = Agent(\n",
    "    name=\"PlannerAgent\",\n",
    "    instructions=INSTRUCTIONS,\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    output_type=WebSearchPlan,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee1550",
   "metadata": {},
   "source": [
    "[User Query]\n",
    "     ↓\n",
    "[QueryClassifierAgent]  ←– Classifies intent (search, research, report, etc.)\n",
    "     ↓\n",
    " ┌──────────────┬────────────────────┬─────────────┐\n",
    " ↓              ↓                    ↓\n",
    "[GoogleSearch] [ResearchAgent]    [ReportGeneratorAgent]\n",
    "                  ↓                        ↓\n",
    "       ┌──────────┬────────────────────┬─────────────────────┐\n",
    "       ↓          ↓                    ↓\n",
    "  [Preprint]  [Multidisciplinary]   [Biomedical & Life Sciences]\n",
    "   (arXiv)     (Scholar, SD, SS)     (PubMed, Springer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Query Classifier Agent \n",
    "2. Google Search Agent\n",
    "3. Research Agent\n",
    "3.1. Preprint Agent\n",
    "3.2. Multidisciplinary Agent\n",
    "3.3. Biomedical Agent\n",
    "4. ReportGeneratorAgent\n",
    "5. PaperContentExtractorAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f84d6",
   "metadata": {},
   "source": [
    "🧭 OVERALL SYSTEM WORKFLOW\n",
    "🟢 Step 1: User Enters a Query\n",
    "The user might ask something like:\n",
    "\n",
    "“Find me research papers on AI in medical imaging.”\n",
    "\n",
    "“What’s the latest on ChatGPT?”\n",
    "\n",
    "“Generate a report on breast cancer detection using AI.”\n",
    "\n",
    "🟡 Step 2: QueryClassifierAgent\n",
    "This agent is the brain of the system at the front.\n",
    "It classifies the query into one of the following intents:\n",
    "\n",
    "Query Type\tAction Taken\n",
    "🔍 Google/Internet search\tRoute to GoogleSearchAgent\n",
    "📄 Research query (papers, articles)\tRoute to ResearchAgent\n",
    "📑 Report generation\tRoute to ReportGeneratorAgent\n",
    "❓ Other queries (optional)\tCan route to FallbackAgent or ask for clarification\n",
    "\n",
    "🔵 Step 3: Path Selection Based on Classification\n",
    "✅ If it's a Google-like query:\n",
    "→ GoogleSearchAgent handles it\n",
    "(Uses Google Search API, Bing API, or web scraping to answer general questions)\n",
    "\n",
    "✅ If it's a Research-related query:\n",
    "→ Route to ResearchAgent\n",
    "This agent will further classify the research query based on domain:\n",
    "\n",
    "Domain\tSub-Agent Called\n",
    "🧪 Preprint / Early-stage Research\tPreprintAgent → (arXiv API)\n",
    "🌐 Multidisciplinary\tMultidisciplinaryAgent → (Scholarly, ScienceDirect, Semantic Scholar)\n",
    "🧬 Biomedical / Life Sciences\tBiomedicalAgent → (PubMed, Springer Nature Open Access API)\n",
    "\n",
    "Each sub-agent:\n",
    "\n",
    "Queries its relevant platform(s)\n",
    "\n",
    "Returns paper results (title, abstract, link)\n",
    "\n",
    "📕 Step 4: If it's a Report Request\n",
    "2 Scenarios:\n",
    "✳️ Case A: User directly asks for a report\n",
    "Example: \"Generate a report on Alzheimer's treatment advances.\"\n",
    "\n",
    "QueryClassifierAgent routes it to ReportGeneratorAgent\n",
    "\n",
    "ReportGeneratorAgent:\n",
    "\n",
    "Analyzes the query\n",
    "\n",
    "Calls the appropriate sub-agent via ResearchAgent\n",
    "\n",
    "Fetches research results\n",
    "\n",
    "Summarizes & organizes them\n",
    "\n",
    "Outputs a structured report (e.g., in Markdown, PDF)\n",
    "\n",
    "✳️ Case B: User first asks for papers, then for a report\n",
    "Example:\n",
    "\n",
    "User: \"Give me papers on blockchain in healthcare\" → routed to ResearchAgent\n",
    "\n",
    "Results are fetched and shown.\n",
    "\n",
    "User: \"Now make a report on that\" → routed to ReportGeneratorAgent\n",
    "\n",
    "ReportGeneratorAgent:\n",
    "\n",
    "Takes the existing paper list (from memory/context)\n",
    "\n",
    "Summarizes, clusters, and formats the content into a report\n",
    "\n",
    "🧱 AGENT RESPONSIBILITIES\n",
    "Agent\tRole\n",
    "QueryClassifierAgent\tClassifies the user's intent\n",
    "GoogleSearchAgent\tHandles general web/internet queries\n",
    "ResearchAgent\tClassifies & routes research queries\n",
    "PreprintAgent\tFetches from arXiv\n",
    "MultidisciplinaryAgent\tFetches from Google Scholar, ScienceDirect, Semantic Scholar\n",
    "BiomedicalAgent\tFetches from PubMed and Springer\n",
    "ReportGeneratorAgent\tSummarizes research and produces structured reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b6a6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
